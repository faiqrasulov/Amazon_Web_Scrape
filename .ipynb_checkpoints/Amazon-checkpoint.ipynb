{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06633b39-cdae-4589-874e-9b5bef7da795",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the page for any captcha or password prompts and proceed manually. Press ENTER to continue.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n",
      "Enter the search term:  laptop\n",
      "Enter the number of pages to scrape:  24\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "\n",
    "# Set up the driver\n",
    "service = Service(executable_path='/Users/FaiqRasulov/Desktop/chromedriver')  # Update path to your chromedriver\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Log in to Amazon\n",
    "login_url = \"https://www.amazon.com/ap/signin?openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.amazon.com%2Fref%3Dnav_ya_signin&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.assoc_handle=usflex&openid.mode=checkid_setup&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&\"\n",
    "driver.get(login_url)\n",
    "\n",
    "# Enter email\n",
    "email_input = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"ap_email\"))\n",
    ")\n",
    "time.sleep(random.uniform(1, 3))\n",
    "email_input.send_keys(\"mail@mail.com)\n",
    "\n",
    "# Click continue\n",
    "time.sleep(random.uniform(1, 3))\n",
    "continue_button = driver.find_element(By.ID, \"continue\")\n",
    "continue_button.click()\n",
    "\n",
    "# Wait for the password input to appear and enter the password\n",
    "password_input = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"ap_password\"))\n",
    ")\n",
    "time.sleep(random.uniform(1, 3))\n",
    "password_input.send_keys(\"pass\")\n",
    "\n",
    "# Check the remember me checkbox\n",
    "time.sleep(random.uniform(1, 3))\n",
    "remember_me_checkbox = driver.find_element(By.NAME, \"rememberMe\")\n",
    "remember_me_checkbox.click()\n",
    "\n",
    "# Click sign in\n",
    "time.sleep(random.uniform(1, 3))\n",
    "sign_in_button = driver.find_element(By.ID, \"signInSubmit\")\n",
    "sign_in_button.click()\n",
    "\n",
    "# Wait for the search bar to load after login\n",
    "WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.ID, \"twotabsearchtextbox\")))\n",
    "\n",
    "# Manual intervention for captcha and re-login, if needed\n",
    "print(\"Please check the page for any captcha or password prompts and proceed manually. Press ENTER to continue.\")\n",
    "input()\n",
    "\n",
    "\n",
    "# Initialize the lists for storing product data\n",
    "product_names = []\n",
    "product_prices = []\n",
    "product_ratings = []\n",
    "delivery_costs = []\n",
    "ships_to = []\n",
    "old_prices = []\n",
    "reviews_counts = []\n",
    "stock_quantities = []\n",
    "more_buying_options = []\n",
    "climate_pledge_friendly = []\n",
    "product_picture_links = []\n",
    "product_options = []\n",
    "product_urls = []\n",
    "product_climate_friendly_info = []\n",
    "delivery_dates = []\n",
    "\n",
    "\n",
    "# Perform the search\n",
    "search_term = input(\"Enter the search term: \")\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(search_term)\n",
    "search_box.submit()\n",
    "\n",
    "num_pages = int(input(\"Enter the number of pages to scrape: \"))\n",
    "page_counter = 1\n",
    "scraped_pages = 0\n",
    "\n",
    "\n",
    "\n",
    "def scrape_data():\n",
    "    products = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@data-component-type='s-search-result']\")))\n",
    "    for product in products:\n",
    "        try:\n",
    "            name = product.find_element(By.XPATH, \".//h2/a\").text\n",
    "        except:\n",
    "            name = \"N/A\"\n",
    "        product_names.append(name)\n",
    "        try:\n",
    "            price = product.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "        except:\n",
    "            price = \"N/A\"\n",
    "        product_prices.append(price)\n",
    "        try:\n",
    "            rating = product.find_element(By.XPATH, \".//span[@class='a-icon-alt']\").get_attribute('innerHTML').split(\" \")[0]\n",
    "        except:\n",
    "            rating = \"N/A\"\n",
    "        product_ratings.append(rating)\n",
    "\n",
    "        try:\n",
    "            fastest_delivery = product.find_element(By.XPATH, \".//div[@class='a-row']//span[@aria-label][1]\")\n",
    "            delivery_costs.append(fastest_delivery.text)\n",
    "        except:\n",
    "            delivery_costs.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            review_count = product.find_element(By.XPATH, \".//a[@class='a-link-normal s-underline-text s-underline-link-text s-link-style']\")\n",
    "            reviews_counts.append(review_count.text.strip())\n",
    "        except:\n",
    "            reviews_counts.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            stock_quantity = product.find_element(By.XPATH, \".//span[contains(text(), 'left in stock')]\")\n",
    "            stock_quantities.append(stock_quantity.text)\n",
    "        except:\n",
    "            stock_quantities.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            climate_friendly_info = product.find_element(By.XPATH, \".//span[contains(text(), 'Climate Pledge Friendly')]\")\n",
    "            climate_pledge_friendly.append(True)\n",
    "            product_climate_friendly_info.append(climate_friendly_info.text)\n",
    "        except:\n",
    "            climate_pledge_friendly.append(False)\n",
    "            product_climate_friendly_info.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            product_picture = product.find_element(By.XPATH, \".//img[@class='s-image']\")\n",
    "            product_picture_links.append(product_picture.get_attribute('src'))\n",
    "        except:\n",
    "            product_picture_links.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            product_information = product.find_elements(By.XPATH, \".//div[contains(@class, 's-product-grid-adjustment')]//span\")\n",
    "            product_info = ', '.join([elem.text for elem in product_information])\n",
    "            product_options.append(product_info)\n",
    "        except:\n",
    "            product_options.append(\"N/A\")\n",
    "\n",
    "        try:\n",
    "            product_url = product.find_element(By.XPATH, \".//h2/a\").get_attribute('href')\n",
    "        except:\n",
    "            product_url = \"N/A\"\n",
    "        product_urls.append(product_url)\n",
    "        try:\n",
    "            old_price_element = product.find_element(By.XPATH, \".//span[contains(@class, 'a-text-price')]\")\n",
    "            old_price_text = old_price_element.text\n",
    "            old_price = re.search(r'\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?', old_price_text).group()\n",
    "            old_prices.append(old_price)\n",
    "        except Exception as e:\n",
    "            old_prices.append(\"N/A\")\n",
    "        try:\n",
    "            delivery_date = product.find_element(By.XPATH, \".//span[contains(@class, 'a-color-base') and contains(@class, 'a-text-bold')]\")\n",
    "            delivery_dates.append(delivery_date.text)\n",
    "        except:\n",
    "            delivery_dates.append(\"N/A\")\n",
    "scrape_data()\n",
    "\n",
    "def infinite_scroll(driver, scroll_pause_time=0.5):\n",
    "    y = 0\n",
    "    max_y = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    increment = max_y // 5  # Adjust the divisor to change the number of increments, lower values mean fewer increments\n",
    "\n",
    "    while y < max_y:\n",
    "        driver.execute_script(f\"window.scrollTo(0, {y});\")\n",
    "        y += increment\n",
    "        time.sleep(scroll_pause_time)\n",
    "\n",
    "qid = None\n",
    "\n",
    "while scraped_pages < num_pages:\n",
    "    if page_counter == 1:\n",
    "        # Scrape the first page\n",
    "        time.sleep(5)  # Give time for the page to load completely\n",
    "        current_page_number = 1\n",
    "    else:\n",
    "        if page_counter <= 20:\n",
    "            # Navigate to the next page using the \"Next\" button\n",
    "            try:\n",
    "                next_button = driver.find_element(By.XPATH, \"//a[contains(@class,'s-pagination-next')]\")\n",
    "                next_button_url = next_button.get_attribute(\"href\")\n",
    "                driver.get(next_button_url)\n",
    "                time.sleep(5)  # Give time for the page to load completely\n",
    "                \n",
    "                if qid is None:\n",
    "                    # Extract the qid parameter from the URL\n",
    "                    url_parts = urlparse(next_button_url)\n",
    "                    url_params = parse_qs(url_parts.query)\n",
    "                    qid = url_params.get(\"qid\", [None])[0]\n",
    "            except NoSuchElementException:\n",
    "                print(f\"Couldn't find the 'Next' button on page {page_counter}\")\n",
    "                break\n",
    "        else:\n",
    "            # Manually construct the URL for other pages\n",
    "            url = f\"https://www.amazon.com/s?k={search_term}&page={page_counter}&qid={qid}&ref=sr_pg_{page_counter}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5)  # Give time for the page to load completely\n",
    "\n",
    "    # Perform infinite scroll on the current page\n",
    "    infinite_scroll(driver)\n",
    "\n",
    "    # Call the scrape_data function for the current page\n",
    "    scrape_data()\n",
    "\n",
    "    # Increment the counters\n",
    "    scraped_pages += 1\n",
    "    page_counter += 1\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "# Create DataFrame with all product information\n",
    "df = pd.DataFrame({\n",
    "    'Product Name': product_names,\n",
    "    'Price': product_prices,\n",
    "    'Rating': product_ratings,\n",
    "    'Fastest Delivery': delivery_costs,\n",
    "    'Review Count': reviews_counts,\n",
    "    'Stock Quantity': stock_quantities,\n",
    "    'Product Information': product_options,\n",
    "    'Climate Pledge Friendly': product_climate_friendly_info,\n",
    "    'Product Picture': product_picture_links,\n",
    "    'Product URL': product_urls,\n",
    "    'Delivery Date': delivery_dates,\n",
    "    'Old Price': old_prices\n",
    "})\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def get_brand_name(product_name):\n",
    "    brand = re.search(r\"^(HP|Apple|Samsung|Dell|Gateway|Lenovo|ASUS|Acer|OEM)\", product_name)\n",
    "    return brand.group(0) if brand else None\n",
    "\n",
    "def get_product_info(product_name):\n",
    "    product_info = re.search(r\"^[A-Za-z]+ (\\d{4} )?(.+?)(,|$)\", product_name)\n",
    "    return product_info.group(2) if product_info else None\n",
    "\n",
    "def get_other_product_info(product_name):\n",
    "    other_product_info = re.search(r\".+?,(.+)\", product_name)\n",
    "    return other_product_info.group(1).strip() if other_product_info else None\n",
    "\n",
    "def clean_product_info(info):\n",
    "    cleaned_info = info.replace(\",\", \"\").replace(\":\", \"\").split()\n",
    "    result = []\n",
    "    for i in range(0, len(cleaned_info), 2):\n",
    "        if i + 1 < len(cleaned_info):\n",
    "            result.append(cleaned_info[i] + \": \" + cleaned_info[i+1])\n",
    "    return \", \".join(result)\n",
    "\n",
    "df['Product Information'] = df['Product Information'].apply(clean_product_info)\n",
    "df['Brand Name'] = df['Product Name'].apply(get_brand_name)\n",
    "df['Product Info'] = df['Product Name'].apply(get_product_info)\n",
    "df['Other Product Info'] = df['Product Name'].apply(get_other_product_info)\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Export the DataFrame to an Excel file with the search term and today's date in the specified directory\n",
    "filename = f\"/Users/FaiqRasulov/Desktop/Qiymet_list/amazon_data_{search_term}_{today}.xlsx\"\n",
    "df.to_excel(filename, index=False)\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac578d02-62db-4c5a-90fb-f6dd31680aa4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c326154-9b17-4b97-921b-0dcf3b1f3894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
